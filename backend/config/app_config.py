"""
Application configuration and database setup.
This module contains all configuration logic and database initialization
for the FastAPI application.
"""
import os
import logging
from sqlalchemy import create_engine, text, inspect
from sqlalchemy.orm import sessionmaker
from contextlib import asynccontextmanager
from dotenv import load_dotenv

# Import LLM service for lifespan
from services.llm_service import llm_service

# Load environment variables
load_dotenv()

# Check if pgvector is available
USE_PGVECTOR = os.getenv("USE_PGVECTOR", "false").lower() == "true"
try:
    from pgvector.sqlalchemy import Vector
    PGVECTOR_AVAILABLE = True
except ImportError:
    PGVECTOR_AVAILABLE = False
    Vector = None
    if USE_PGVECTOR:
        logging.warning("pgvector not installed. Install with: pip install pgvector. Falling back to JSON text storage.")

# CORS configuration
# Cho ph√©p c·∫•u h√¨nh CORS origins qua bi·∫øn m√¥i tr∆∞·ªùng
# Format: CORS_ORIGINS=http://localhost:8000,http://localhost:3000,https://yourdomain.com
CORS_ORIGINS_ENV = os.getenv("CORS_ORIGINS", "")
if CORS_ORIGINS_ENV:
    # Parse t·ª´ env variable (comma-separated)
    ALLOWED_ORIGINS = [origin.strip() for origin in CORS_ORIGINS_ENV.split(",") if origin.strip()]
else:
    # Default: ch·ªâ cho ph√©p localhost cho development
    ALLOWED_ORIGINS = [
        "http://localhost:8000",
        "http://localhost:3000",
        "http://127.0.0.1:8000",
        "http://127.0.0.1:3000",
    ]

# Database configuration
# JDBC URL: jdbc:postgresql://192.168.0.106:5432/ai_system
# Convert to Python format: postgresql://user:password@host:port/database
DB_HOST = os.getenv("DB_HOST", "192.168.0.106")
DB_PORT = os.getenv("DB_PORT", "5432")
DB_NAME = os.getenv("DB_NAME", "ai_system")
DB_USER = os.getenv("DB_USER", "postgres")
DB_PASSWORD = os.getenv("DB_PASSWORD", "")

# TLS/SSL configuration for database
DB_SSL_MODE = os.getenv("DB_SSL_MODE", "prefer")  # disable, allow, prefer, require, verify-ca, verify-full
DB_SSL_ROOT_CERT = os.getenv("DB_SSL_ROOT_CERT", None)  # Path to CA certificate
DB_SSL_CERT = os.getenv("DB_SSL_CERT", None)  # Path to client certificate
DB_SSL_KEY = os.getenv("DB_SSL_KEY", None)  # Path to client key

# Build database URL with SSL parameters
DATABASE_URL = f"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}"

# SSL connection arguments for SQLAlchemy
DB_CONNECT_ARGS = {
    "connect_timeout": 10,
    "sslmode": DB_SSL_MODE
}

# Add SSL certificates if provided
if DB_SSL_ROOT_CERT:
    DB_CONNECT_ARGS["sslrootcert"] = DB_SSL_ROOT_CERT
if DB_SSL_CERT:
    DB_CONNECT_ARGS["sslcert"] = DB_SSL_CERT
if DB_SSL_KEY:
    DB_CONNECT_ARGS["sslkey"] = DB_SSL_KEY


def sanitize_database_url(url: str) -> str:
    """
    Sanitize database URL ƒë·ªÉ lo·∫°i b·ªè password khi log
    Thay password b·∫±ng '***' ƒë·ªÉ b·∫£o m·∫≠t
    """
    try:
        from urllib.parse import urlparse, urlunparse
        
        parsed = urlparse(url)
        # Gi·ªØ nguy√™n scheme, netloc (nh∆∞ng mask password), path, params, fragment
        # Ch·ªâ thay ƒë·ªïi ph·∫ßn password trong netloc
        if parsed.password:
            # Thay password b·∫±ng ***
            netloc = parsed.netloc.replace(f":{parsed.password}@", ":***@")
        else:
            netloc = parsed.netloc
        
        sanitized = urlunparse((
            parsed.scheme,
            netloc,
            parsed.path,
            parsed.params,
            parsed.query,
            parsed.fragment
        ))
        return sanitized
    except Exception:
        # N·∫øu c√≥ l·ªói, tr·∫£ v·ªÅ safe version
        return url.split("@")[0] + "@***" if "@" in url else "***"


# Logging: structured logging v·ªõi JSON format (n·∫øu c·∫ßn)
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO").upper()
LOG_FORMAT = os.getenv("LOG_FORMAT", "standard")  # standard ho·∫∑c json

if LOG_FORMAT == "json":
    import json
    from datetime import datetime
    
    class JSONFormatter(logging.Formatter):
        def format(self, record):
            log_data = {
                "timestamp": datetime.utcnow().isoformat(),
                "level": record.levelname,
                "logger": record.name,
                "message": record.getMessage(),
                "module": record.module,
                "function": record.funcName,
                "line": record.lineno
            }
            if record.exc_info:
                log_data["exception"] = self.formatException(record.exc_info)
            return json.dumps(log_data)
    
    handler = logging.StreamHandler()
    handler.setFormatter(JSONFormatter())
    logging.basicConfig(level=getattr(logging, LOG_LEVEL), handlers=[handler])
else:
    # Standard logging format
    logging.basicConfig(
        level=getattr(logging, LOG_LEVEL),
        format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S"
    )

logging.getLogger("sqlalchemy.engine").setLevel(logging.WARNING)

# Import database configuration module
from services.database_config import get_database_config
from services.async_database_config import get_async_database_config

# Create database engine v·ªõi connection pooling configuration ƒë∆∞·ª£c t·ªëi ∆∞u
db_config = get_database_config()
engine = db_config.create_engine()

# Create session factory
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

# Create async database engine v√† session factory
async_db_config = get_async_database_config()
async_engine = async_db_config.create_async_engine()
AsyncSessionLocal = async_db_config.create_async_session_factory(async_engine)


def index_exists(conn, table_name: str, index_name: str) -> bool:
    """Ki·ªÉm tra xem index ƒë√£ t·ªìn t·∫°i ch∆∞a"""
    result = conn.execute(text("""
        SELECT EXISTS (
            SELECT 1 
            FROM pg_indexes 
            WHERE tablename = :table_name 
            AND indexname = :index_name
        )
    """), {"table_name": table_name, "index_name": index_name})
    return result.scalar()


def table_exists(conn, table_name: str) -> bool:
    """Ki·ªÉm tra xem table ƒë√£ t·ªìn t·∫°i ch∆∞a"""
    result = conn.execute(text("""
        SELECT EXISTS (
            SELECT FROM information_schema.tables 
            WHERE table_name = :table_name
        )
    """), {"table_name": table_name})
    return result.scalar()


def setup_cache_entries_table():
    """
    T·ª± ƒë·ªông t·∫°o b·∫£ng cache_entries cho L3 cache
    Ch·∫°y khi ·ª©ng d·ª•ng kh·ªüi ƒë·ªông n·∫øu AUTO_MIGRATE_CACHE_TABLE=true (default: true)
    """
    auto_migrate = os.getenv("AUTO_MIGRATE_CACHE_TABLE", "true").lower() == "true"
    if not auto_migrate:
        logging.info("‚è≠Ô∏è  Auto-migrate cache_entries table disabled (AUTO_MIGRATE_CACHE_TABLE=false)")
        return
    
    try:
        with engine.connect() as conn:
            # Check if table exists
            if table_exists(conn, "cache_entries"):
                logging.debug("‚è≠Ô∏è  Table cache_entries ƒë√£ t·ªìn t·∫°i, b·ªè qua")
                return
            
            # Create table
            create_table = text("""
                CREATE TABLE cache_entries (
                    id SERIAL PRIMARY KEY,
                    cache_key VARCHAR(512) UNIQUE NOT NULL,
                    cache_value TEXT NOT NULL,
                    cache_type VARCHAR(50) NOT NULL,
                    expires_at TIMESTAMP NOT NULL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    access_count INTEGER DEFAULT 0,
                    last_accessed TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            conn.execute(create_table)
            conn.commit()
            logging.info("‚úÖ ƒê√£ t·∫°o b·∫£ng cache_entries cho L3 cache")
            
            # Create indexes
            indexes_to_create = [
                {
                    "name": "idx_cache_entries_key",
                    "table": "cache_entries",
                    "columns": "cache_key",
                    "description": "Index cho cache_key ƒë·ªÉ lookup nhanh"
                },
                {
                    "name": "idx_cache_entries_type",
                    "table": "cache_entries",
                    "columns": "cache_type",
                    "description": "Index cho cache_type ƒë·ªÉ filter theo lo·∫°i cache"
                },
                {
                    "name": "idx_cache_entries_expires",
                    "table": "cache_entries",
                    "columns": "expires_at",
                    "description": "Index cho expires_at ƒë·ªÉ cleanup expired entries"
                },
                {
                    "name": "idx_cache_entries_access_count",
                    "table": "cache_entries",
                    "columns": "access_count",
                    "description": "Index cho access_count ƒë·ªÉ cache warming"
                },
                {
                    "name": "idx_cache_entries_last_accessed",
                    "table": "cache_entries",
                    "columns": "last_accessed",
                    "description": "Index cho last_accessed ƒë·ªÉ cache warming"
                },
            ]
            
            created_indexes = 0
            for idx in indexes_to_create:
                try:
                    if index_exists(conn, idx["table"], idx["name"]):
                        logging.debug(f"‚è≠Ô∏è  Index {idx['name']} ƒë√£ t·ªìn t·∫°i, b·ªè qua")
                        continue
                    
                    create_index_sql = text(f"""
                        CREATE INDEX {idx['name']} 
                        ON {idx['table']} ({idx['columns']})
                    """)
                    
                    conn.execute(create_index_sql)
                    conn.commit()
                    logging.info(f"‚úÖ ƒê√£ t·∫°o index: {idx['name']} tr√™n {idx['table']}({idx['columns']})")
                    created_indexes += 1
                except Exception as e:
                    logging.error(f"‚ùå L·ªói khi t·∫°o index {idx['name']}: {e}")
                    conn.rollback()
            
            if created_indexes > 0:
                logging.info(f"üìä Cache table indexes: ‚úÖ ƒê√£ t·∫°o {created_indexes} indexes")
                
                # Analyze table ƒë·ªÉ PostgreSQL c·∫≠p nh·∫≠t statistics
                logging.info("üîÑ ƒêang ch·∫°y ANALYZE cache_entries ƒë·ªÉ c·∫≠p nh·∫≠t statistics...")
                try:
                    conn.execute(text("ANALYZE cache_entries"))
                    conn.commit()
                except Exception as e:
                    logging.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ analyze b·∫£ng cache_entries: {e}")
    
    except Exception as e:
        logging.error(f"‚ùå L·ªói khi setup cache_entries table: {e}")
        # Kh√¥ng raise exception ƒë·ªÉ app v·∫´n c√≥ th·ªÉ kh·ªüi ƒë·ªông n·∫øu table kh√¥ng th·ªÉ t·∫°o


def setup_database_indexes():
    """
    T·ª± ƒë·ªông t·∫°o c√°c indexes c·∫ßn thi·∫øt cho query optimization
    Ch·∫°y khi ·ª©ng d·ª•ng kh·ªüi ƒë·ªông n·∫øu AUTO_MIGRATE_INDEXES=true (default: true)
    """
    auto_migrate = os.getenv("AUTO_MIGRATE_INDEXES", "true").lower() == "true"
    if not auto_migrate:
        logging.info("‚è≠Ô∏è  Auto-migrate indexes disabled (AUTO_MIGRATE_INDEXES=false)")
        return
    
    indexes_to_create = [
        # Indexes cho agent_conversations
        {
            "name": "idx_agent_conversations_session_id",
            "table": "agent_conversations",
            "columns": "session_id",
            "description": "Index cho session_id ƒë·ªÉ query conversations theo session nhanh h∆°n"
        },
        {
            "name": "idx_agent_conversations_created_at",
            "table": "agent_conversations",
            "columns": "created_at",
            "description": "Index cho created_at ƒë·ªÉ sort v√† filter theo th·ªùi gian nhanh h∆°n"
        },
        {
            "name": "idx_agent_conversations_session_created",
            "table": "agent_conversations",
            "columns": "session_id, created_at",
            "description": "Composite index cho session_id v√† created_at (th∆∞·ªùng query c√πng l√∫c)"
        },
        
        # Indexes cho conversation_feedback
        {
            "name": "idx_conversation_feedback_conversation_id",
            "table": "conversation_feedback",
            "columns": "conversation_id",
            "description": "Index cho conversation_id ƒë·ªÉ join v√† filter feedback theo conversation"
        },
        {
            "name": "idx_conversation_feedback_rating",
            "table": "conversation_feedback",
            "columns": "rating",
            "description": "Index cho rating ƒë·ªÉ filter feedback theo rating nhanh h∆°n"
        },
        {
            "name": "idx_conversation_feedback_conv_rating",
            "table": "conversation_feedback",
            "columns": "conversation_id, rating",
            "description": "Composite index cho conversation_id v√† rating (th∆∞·ªùng filter c√πng l√∫c)"
        },
        
        # Indexes cho conversation_embeddings
        {
            "name": "idx_conversation_embeddings_conversation_id",
            "table": "conversation_embeddings",
            "columns": "conversation_id",
            "description": "Index cho conversation_id ƒë·ªÉ join embeddings v·ªõi conversations nhanh h∆°n"
        },
    ]
    
    try:
        with engine.connect() as conn:
            created_count = 0
            skipped_count = 0
            
            for idx in indexes_to_create:
                try:
                    # Ki·ªÉm tra xem index ƒë√£ t·ªìn t·∫°i ch∆∞a
                    if index_exists(conn, idx["table"], idx["name"]):
                        logging.debug(f"‚è≠Ô∏è  Index {idx['name']} ƒë√£ t·ªìn t·∫°i, b·ªè qua")
                        skipped_count += 1
                        continue
                    
                    # Ki·ªÉm tra xem b·∫£ng c√≥ t·ªìn t·∫°i kh√¥ng
                    if idx["table"] not in inspect(engine).get_table_names():
                        logging.warning(f"‚ö†Ô∏è  B·∫£ng {idx['table']} kh√¥ng t·ªìn t·∫°i, b·ªè qua index {idx['name']}")
                        skipped_count += 1
                        continue
                    
                    # T·∫°o index
                    create_sql = f"""
                        CREATE INDEX {idx['name']} 
                        ON {idx['table']} ({idx['columns']})
                    """
                    
                    conn.execute(text(create_sql))
                    conn.commit()
                    
                    logging.info(f"‚úÖ ƒê√£ t·∫°o index: {idx['name']} tr√™n {idx['table']}({idx['columns']})")
                    created_count += 1
                    
                except Exception as e:
                    logging.error(f"‚ùå L·ªói khi t·∫°o index {idx['name']}: {e}")
                    conn.rollback()
            
            if created_count > 0 or skipped_count > 0:
                logging.info(f"üìä Database indexes: ‚úÖ ƒê√£ t·∫°o {created_count}, ‚è≠Ô∏è  ƒê√£ b·ªè qua {skipped_count}")
            
            # Analyze tables ƒë·ªÉ PostgreSQL c·∫≠p nh·∫≠t statistics
            if created_count > 0:
                logging.info("üîÑ ƒêang ch·∫°y ANALYZE ƒë·ªÉ c·∫≠p nh·∫≠t statistics...")
                for idx in indexes_to_create:
                    try:
                        conn.execute(text(f"ANALYZE {idx['table']}"))
                    except Exception as e:
                        logging.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ analyze b·∫£ng {idx['table']}: {e}")
                conn.commit()
    
    except Exception as e:
        logging.error(f"‚ùå L·ªói khi setup database indexes: {e}")
        # Kh√¥ng raise exception ƒë·ªÉ app v·∫´n c√≥ th·ªÉ kh·ªüi ƒë·ªông n·∫øu indexes kh√¥ng th·ªÉ t·∫°o

# Import models from models.py to avoid circular imports
from .models import (
    Base, AgentTask, AgentConversation, ConversationFeedback, 
    ConversationEmbedding, APIKey, APIKeyAuditLog, CacheEntry
)

# Create tables
Base.metadata.create_all(bind=engine)

# Import Pydantic models from separate module for better organization
# Re-export for backward compatibility
from .pydantic_models import (
    TaskCreate,
    TaskResponse,
    ConversationCreate,
    ConversationResponse,
    FeedbackCreate,
    FeedbackResponse,
    FeedbackStats,
)

# Lifespan event handler (thay th·∫ø on_event deprecated)
@asynccontextmanager
async def lifespan(app):
    # Startup
    try:
        with engine.connect() as conn:
            conn.execute(text("SELECT 1"))
        logging.info("Database connection: OK")
        
        # Setup database indexes t·ª± ƒë·ªông
        setup_database_indexes()
        
        # Setup cache_entries table t·ª± ƒë·ªông
        setup_cache_entries_table()
        
        # Check Ollama connection
        ollama_status = await llm_service.check_ollama_connection()
        if ollama_status.get("connected"):
            exact_model = ollama_status.get("exact_model", llm_service.model_name)
            logging.info(f"Ollama connection: OK - Model: {exact_model}")
            if not ollama_status.get("model_available"):
                available_models = ollama_status.get('models', [])
                logging.warning(f"Model '{llm_service.model_name}' not found in Ollama. Available models: {available_models}")
                # G·ª£i √Ω model name ƒë√∫ng
                if available_models:
                    suggested_model = available_models[0]
                    logging.info(f"G·ª£i √Ω: S·ª≠ d·ª•ng model '{suggested_model}' (c·∫≠p nh·∫≠t LLM_MODEL_NAME trong .env)")
        else:
            logging.warning(f"Ollama connection failed: {ollama_status.get('error', 'Unknown error')}")
        
        # Start background tasks
        from services.background_tasks import background_tasks_service
        await background_tasks_service.start()
        logging.info("Background tasks started")
        
        # Initialize cache service ƒë·ªÉ test Redis connection khi app start
        try:
            from services.advanced_cache_service import get_advanced_cache_service
            cache_service = get_advanced_cache_service()
            if cache_service.l2_enabled:
                logging.info("Redis cache service initialized and connected")
            else:
                logging.debug("Cache service initialized (Redis not available or disabled)")
        except Exception as e:
            logging.debug(f"Cache service initialization skipped: {e}")
        
        # Start embedding precompute task n·∫øu ƒë∆∞·ª£c b·∫≠t
        try:
            from services.embedding_service import embedding_service
            if embedding_service.precompute_enabled:
                precompute_interval = int(os.getenv("EMBEDDING_PRECOMPUTE_INTERVAL", "3600"))  # Default: 1 hour
                await embedding_service.start_precompute_task(precompute_interval)
                logging.info(f"Embedding precompute task started (interval: {precompute_interval}s)")
        except Exception as e:
            logging.debug(f"Embedding precompute task initialization skipped: {e}")
        
        # Start Celery worker n·∫øu ENABLE_CELERY_WORKER=true
        enable_celery_worker = os.getenv("ENABLE_CELERY_WORKER", "false").lower() == "true"
        if enable_celery_worker:
            try:
                from services.celery_worker_manager import start_celery_worker
                start_celery_worker()
                logging.info("‚úÖ Celery worker integrated and started")
            except Exception as e:
                logging.warning(f"‚ö†Ô∏è  Failed to start integrated Celery worker: {e}")
                logging.info("üí° You can still use external Celery worker with: celery -A services.celery_config worker")
    except Exception as exc:
        # Kh√¥ng log exception tr·ª±c ti·∫øp v√¨ c√≥ th·ªÉ ch·ª©a password
        # Ch·ªâ log error message an to√†n
        error_msg = str(exc)
        # Lo·∫°i b·ªè password n·∫øu c√≥ trong error message
        if "password" in error_msg.lower() or "@" in error_msg or "postgresql://" in error_msg:
            error_msg = "Database connection failed. Please check database configuration."
        logging.error("Database connection failed: %s", error_msg)
        # Log sanitized database URL ƒë·ªÉ debug (kh√¥ng c√≥ password)
        logging.debug("Database URL (sanitized): %s", sanitize_database_url(DATABASE_URL))
    
    yield
    
    # Shutdown
    try:
        from services.background_tasks import background_tasks_service
        await background_tasks_service.stop()
        logging.info("Background tasks stopped")
    except Exception as e:
        logging.debug(f"Error stopping background tasks: {e}")
    
    try:
        from services.embedding_service import embedding_service
        embedding_service.stop_precompute_task()
        logging.info("Embedding precompute task stopped")
    except Exception as e:
        logging.debug(f"Error stopping embedding precompute task: {e}")
        logging.error(f"Error stopping background tasks: {e}")
    
    # Stop Celery worker n·∫øu ƒëang ch·∫°y (ch·∫°y trong finally ƒë·ªÉ ƒë·∫£m b·∫£o lu√¥n ƒë∆∞·ª£c g·ªçi)
    try:
        from services.celery_worker_manager import stop_celery_worker
        stop_celery_worker()
    except Exception as e:
        logging.error(f"Error stopping Celery worker: {e}")